{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. File description \n",
    "\n",
    "To feed a model for it to predict a bot from a legit users I need the data to be labelled meaning each user in the dataset needs to have a 1 or 0 for bot/not bot. This file is meant to do so.\n",
    "\n",
    "To label the data, multiple techniques are gonna be used and are divided in three. Manual, semi-manual and automatic labelling. \n",
    "\n",
    "Automatic: \n",
    "* Look at bot labelled and check for other users with the same photo. From previous iteration of the project, I have a folder with about 800 of bots photos that I previously found manually. Now, I just check if a user's profile pic is similar to any, if yes, it's a bot. That's the easiest way and the only one where I don't have to check other details of the bot as if two users have the same photo and one is a bot, the other is too.\n",
    "\n",
    "Semi-manual:\n",
    "* Look at the bots having a scam domain.\n",
    "* Use NSFW pre-trained models to get images score for Neutral, Drawing, Hentai, Sexual, Porn and looking at the profile that have a high score for porn + sexy as most bots have nude photos\n",
    "* Look at a subset of data where I belive bots are\n",
    "    * Around 200 likes on their comment\n",
    "    * 12 or less photos\n",
    "    * Always have a link in their bio\n",
    "* Find same/similar comments from labelled users \n",
    "* Pattern in Bio/description\n",
    "* Look at the keyword of the flagged bots in comments and profile, look at profile that are similar to bots. \n",
    "* After labelling abunch of users, build a weak model to help in the labelling by making predictions. As long as the model is better than random, it'll be helpful.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import module and setting notebook preferences\n",
    "\n",
    "### 0.1 Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'demoji'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39museful\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msu\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m \n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/code/Instagram_bot_classification/src/useful.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mos\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpath\u001b[39;00m \u001b[39mimport\u001b[39;00m isfile, join\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mdemoji\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlist_files\u001b[39m(directory, extension\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, path\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, replace\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, subdir\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, common\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'demoji'"
     ]
    }
   ],
   "source": [
    "import src.useful as su\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import sqlite3 \n",
    "\n",
    "import plotly.io as pio\n",
    "\n",
    "from imagededup.methods import PHash\n",
    "from PIL import Image, ImageDraw, ImageFont, UnidentifiedImageError\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import joblib\n",
    "\n",
    "from tqdm import tqdm \n",
    "\n",
    "from IPython.display import Image as IImage, display, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Notebook preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "\n",
    "# Creating SQL database to store all the data for the project\n",
    "database = \"data/main_database.sqlite\"\n",
    "con = sqlite3.connect(database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Manual labelling \n",
    "\n",
    "To label the data manually, the process has to be fast, I need to see all the information about the users condensed in one summary. For that, I chose to create for each user an image with their attributes and their photos. I'll then create a script to open one image by one, and then chose bot/not bot/maybe bot with keyboard arrows. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and display only the one from a \n",
    "query = '''\n",
    "select \n",
    "    * \n",
    "from clean_comments_users_last12\n",
    "'''\n",
    "\n",
    "df_main = pd.read_sql_query(query, con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to create an empty dataframe for the table labels\n",
    "\n",
    "# df = pd.read_csv('/Users/marclamy/Desktop/code/all ig bot folders/igbot_final/data/labelled_usernames_method2_temp.csv')\n",
    "# df.head(0).to_sql('labels', con, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Generate summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to use the profile of a bot for the image demo. The account is still live but hasn't been active for about 9 months, at last the last comment on their post is. This is the developed function from [generate_summary.py](https://github.com/marclelamy/instagram_bot_classification/blob/main/3.1_generate_summary.py)\n",
    "\n",
    "First, create a blank image, define font."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_image = Image.new(\"RGB\", (1300, 450), \"black\")\n",
    "draw = ImageDraw.Draw(global_image)\n",
    "font = ImageFont.truetype(\"assets/OpenSans-Light.ttf\", 15)\n",
    "\n",
    "global_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the profile picture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'accurate-eccentric-lion'\n",
    "df_user = df_main[df_main[\"username\"]==username].reset_index(drop=True)\n",
    "\n",
    "# Open user pp and add it to the global image\n",
    "try:\n",
    "    path = f\"data/photos/user_profile_pictures/{username}_pp_user_photo.png\"\n",
    "    profile_pic = Image.open(path)\n",
    "    global_image.paste(profile_pic, (250, 0))\n",
    "except (FileNotFoundError, UnidentifiedImageError): \n",
    "    pass\n",
    "\n",
    "global_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding twelve other photos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of position to add the photos on the global image\n",
    "positions = []\n",
    "for x in range(850, 1300, 150):\n",
    "    for y in range(0, 450, 150):\n",
    "        positions.append((x, y))\n",
    "\n",
    "\n",
    "# Loop through each image and add it to the global image\n",
    "for image_num in range(12):\n",
    "    path = f\"data/photos/user_last_12_posts/{username}_{str(image_num)}_user_photo.png\"\n",
    "\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            img = Image.open(path)\n",
    "            if image_num < 9:\n",
    "                global_image.paste(img.resize((150, 150)), positions[image_num])\n",
    "            elif image_num == 9:\n",
    "                global_image.paste(img.resize((150, 150)), (400, 0))\n",
    "            elif image_num == 10:   \n",
    "                global_image.paste(img.resize((150, 150)), (550, 0))\n",
    "            elif image_num == 11:\n",
    "                global_image.paste(img.resize((150, 150)), (700, 0))\n",
    "    except (OSError, UnidentifiedImageError) as e: # For truncated image file\n",
    "        pass\n",
    "\n",
    "global_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding screenshot of the landing page of the url in bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open screenshot, resize it, add it to global_image \n",
    "path = f\"data/photos/bio_url_screenshot/{username}_website_photo.png\"\n",
    "try:\n",
    "    screenshot = Image.open(path)\n",
    "    global_image.paste(screenshot.resize((500, 300)), (350, 150))\n",
    "except (UnidentifiedImageError, FileNotFoundError):\n",
    "    pass\n",
    "\n",
    "global_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add basic information \n",
    "username = df_user.loc[0, \"username\"]\n",
    "draw.text((10, 5), f\"Username: {username}\", font=font, fill=\"white\")\n",
    "\n",
    "follower_count = df_user.loc[0, \"follower_count\"]\n",
    "color = \"white\" if follower_count > 800 else \"red\"\n",
    "draw.text((10, 25), f\"Followers: {follower_count:,}\", font=font, fill=color)\n",
    "\n",
    "follow_count = df_user.loc[0, \"follow_count\"]\n",
    "draw.text((10, 45), f\"Following: {follow_count:,}\", font=font, fill=\"white\")\n",
    "\n",
    "post_count = df_user.loc[0, \"post_count\"]\n",
    "color = \"white\" if post_count > 16 else \"red\"\n",
    "draw.text((10, 65), f\"Post count: {post_count}\", font=font, fill=color)\n",
    "\n",
    "video_count = df_user.loc[0, \"video_count\"]\n",
    "color = \"white\" if video_count > 0 else \"red\"\n",
    "draw.text((10, 85), f\"Video count: {video_count}\", font=font, fill=color)\n",
    "\n",
    "comment_likes = round(df_user[\"comment_likes\"].mean())\n",
    "color = \"white\" if 200 <= comment_likes < 450 else \"red\"\n",
    "draw.text((10, 105), f\"Comment_likes: {comment_likes}\", font=font, fill=\"white\")\n",
    "\n",
    "domain = df_user.loc[0, \"domain\"]\n",
    "draw.text((10, 125), f\"{domain}\", font=font, fill=\"white\")\n",
    "\n",
    "biography = df_user.loc[0, \"biography\"]\n",
    "draw.text((10, 145), f\"{biography}\", font=font, fill=\"white\")\n",
    "\n",
    "\n",
    "global_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary_path = f\"data/photos/image_summary/{username}_image_summary.png\"\n",
    "# global_image.save(summary_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Manual labelling one by one\n",
    "\n",
    "This is the process of labelling the users one by one, twice to eliminate human error. For this I made a script to open each image summary generated in the precedent step and label it with the arrows: `{'right': 'bot', 'up': 'legit', 'down': 'maybe'}`.\n",
    "\n",
    "Look up for [3.2_manual_labelling.py](https://github.com/marclelamy/instagram_bot_classification/blob/main/3.2_manual_labelling.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Semi-manual labelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image dupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # list all the photos folders\n",
    "# last12_folder = 'data/photos/user_last_12_posts'\n",
    "# pp_folder = 'data/photos/user_profile_pictures'\n",
    "# allphotos_folder = 'data/photos/all_photos'\n",
    "\n",
    "\n",
    "# phasher = PHash()\n",
    "# folder_paths = [pp_folder, last12_folder, allphotos_folder]\n",
    "# # folder_paths = ['test_folder_photos']\n",
    "# for folder_path in tqdm(folder_paths): \n",
    "#     file_name = folder_path.split('/')[-1]\n",
    "#     print(f'data/photos/{file_name}_encodings.pkl')\n",
    "#     if os.path.isfile(f'data/photos/{file_name}_encodings.pkl'):\n",
    "#         encodings = joblib.load(f'data/photos/{file_name}_encodings.pkl')\n",
    "#     else: \n",
    "#         encodings = phasher.encode_images(image_dir=folder_path)\n",
    "    \n",
    "#     if os.path.isfile(f'data/photos/{file_name}_duplicates.pkl') == False:\n",
    "#         duplicates = phasher.find_duplicates(encoding_map=encodings)\n",
    "#         joblib.dump(duplicates, f'data/photos/{file_name}_duplicates.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pp_path_duplicated = joblib.load(f'data/photos/user_profile_pictures_light_duplicates.pkl')\n",
    "# last12_path_duplicated = joblib.load('data/photos/user_profile_pictures_light_duplicates.pkl')\n",
    "all_path_duplicated = joblib.load('data/photos/all_photos_duplicates.pkl')\n",
    "\n",
    "# print(len(all_path_duplicated), len([key for key, value in all_path_duplicated.items() if len(value) > 0]), len([[key, value] for key, values in all_path_duplicated.items() for value in values]))\n",
    "\n",
    "\n",
    "# def dupeless (dupes): \n",
    "#     for key, values in list(dupes.items()):\n",
    "#         if key in dupes.keys():\n",
    "#             if len(values) == 0:\n",
    "#                 del dupes[key]\n",
    "#                 continue\n",
    "#             for value in values:\n",
    "#                 if value in dupes.keys():\n",
    "#                     del dupes[value]\n",
    "\n",
    "#     return dupes \n",
    "\n",
    "# du = dupeless(all_path_duplicated)\n",
    "# import sys\n",
    "# print(sys.getsizeof(du), sys.getsizeof(du) / sys.getsizeof(all_path_duplicated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len([[key, value] for key, values in all_path_duplicated.items() for value in values]), len([[key, value] for key, values in du.items() for value in values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dupeless (dupes): \n",
    "    for key, values in list(dupes.items()):\n",
    "        if key in dupes.keys():\n",
    "            if len(values) == 0:\n",
    "                del dupes[key]\n",
    "                continue\n",
    "            for value in values:\n",
    "                if value in dupes.keys():\n",
    "                    del dupes[value]\n",
    "\n",
    "    return dupes \n",
    "\n",
    "print(len(all_path_duplicated.keys()))\n",
    "all_path_duplicated = dupeless(all_path_duplicated)\n",
    "print(len(all_path_duplicated.keys()))\n",
    "\n",
    "\n",
    "all_values = [''.join(sorted([key, value])) for key, values in all_path_duplicated.items() for value in values]\n",
    "len(set(all_values)) == len(all_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos_dupes = [sorted([key, value]) for key, values in tqdm(all_path_duplicated.items()) for value in values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the type of the photo to only keep the username and remove the duplicates on the username/username_dupe subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict(pd.read_sql_query('select cooler_name, username from username_mapping', con).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# username_mapping = dict(pd.read_sql_query('select username, cooler_name from username_mapping', con).values)\n",
    "\n",
    "# def rmv_photo_type(text): \n",
    "#     original_text = text\n",
    "#     for replacement in [f'_{num}_user_photo.png' for num in range(12)] + ['_pp_user_photo.png']:\n",
    "#         text = text.replace(replacement, '')\n",
    "\n",
    "#     return original_text.replace(text, username_mapping[text])\n",
    "\n",
    "\n",
    "# for index, value in enumerate(photos_dupes):\n",
    "#     photos_dupes[index] = [rmv_photo_type(value[0]), rmv_photo_type(value[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create photos dupes dataframe\n",
    "df_photos_dupes = pd.DataFrame(photos_dupes, columns=['photo_name', 'photo_name_dupe'])\n",
    "\n",
    "# Remove duplicate combinations\n",
    "print(f'Before removing dupes: {df_photos_dupes.shape}')\n",
    "df_photos_dupes = df_photos_dupes.drop_duplicates()\n",
    "print(f'After removing dupes: {df_photos_dupes.shape}')\n",
    "\n",
    "df_photos_dupes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding and ID for each group "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding an ID for all similar photos\n",
    "photos_names = df_photos_dupes.groupby('photo_name', as_index=False).count().sort_values(['photo_name_dupe', 'photo_name'], ascending=False)['photo_name']\n",
    "image_group_mapping = {photo_name: index for index, photo_name in enumerate(photos_names)}\n",
    "df_photos_dupes['groupid'] = df_photos_dupes['photo_name'].apply(lambda x: image_group_mapping[x])\n",
    "df_photos_dupes.to_sql('photos_dupes', con, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_photos_dupes.groupby('photo_name', as_index=False).count().sort_values(['photo_name_dupe', 'photo_name'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_photos_dupes.query('photo_name == \"abiding-beige-hound_pp_user_photo.png\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X.x Creating function to see group photo and stor them properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show image of specific category\n",
    "def show_photo_group(groupid):\n",
    "    df_photos_dupes_group_category = df_photos_dupes.query(f'groupid == {groupid}')[['photo_name', 'photo_name_dupe']]\n",
    "    photos_dupes_group_category = [x for l in df_photos_dupes_group_category.values for x in l]\n",
    "    print(groupid, len(photos_dupes_group_category))\n",
    "    return IImage(filename=f'data/photos/all_photos/' + photos_dupes_group_category[0])\n",
    "\n",
    "# show_photo_group(36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_groupid_photo_together(category):\n",
    "    df_photos_dupes_group_category = df_photos_dupes.query(f'groupid == {category}')[['photo_name', 'photo_name_dupe']]\n",
    "    photos_dupes_group_category = {x for l in df_photos_dupes_group_category.values for x in l}\n",
    "    # print(category, len(photos_dupes_group_category))\n",
    "\n",
    "    # Check if folder exists, otherwise create it\n",
    "    dir_copy = f'data/photos/categories/{category}/'\n",
    "    if os.path.isdir(dir_copy):\n",
    "        for file in su.list_files(dir_copy, path=True):\n",
    "            os.remove(file)\n",
    "    else:\n",
    "        os.makedirs(dir_copy)\n",
    "\n",
    "    # Copying each image into the new directory\n",
    "    for photo in photos_dupes_group_category:\n",
    "        try:\n",
    "            shutil.copy('data/photos/all_photos/' + photo, dir_copy + photo)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    \n",
    "    shutil.copy('data/photos/all_photos/' + photo, 'data/photos/categories/00unique/' + str(category) + '.png')\n",
    "    # display(IImage(filename=dir_copy + photo))\n",
    "\n",
    "\n",
    "for groupid in range(5):\n",
    "    # display(show_photo_group(groupid))\n",
    "    store_groupid_photo_together(groupid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 4554\n",
    "df_photos__dupes_group_4554 = df_photos_dupes.query(f'photo_combo_group == {category}')[['photo_name', 'photo_name_dupe']]\n",
    "photos__dupes_group_4554 = {x for l in df_photos__dupes_group_4554.values for x in l}\n",
    "\n",
    "for photo in photos__dupes_group_4554:\n",
    "    shutil.copy('data/photos/all_photos/' + photo, f'data/photos/categories/{category}/' + photo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos__dupes_group_4554"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_photos_dupes[['photo_name','photo_name_dupe']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_photos_dupes = df_photos_dupes[['username', 'username_dupe', 'photo_name', 'photo_name_dupe']]\n",
    "df_photos_dupes.to_sql('photos_dupe', con, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query('select * from clean_users', con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.0 Create file with photos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.shuffle(last12_photos_paths)\n",
    "for path in tqdm(last12_photos_paths[:50000]): \n",
    "    shutil.copy(path, path.replace('data/photos/user_last_12_posts/', 'test_folder_photos/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path.replace('data/photos/user_last_12_posts/', 'test_folder_photos/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_files('test_folder_photos'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Automatic labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
