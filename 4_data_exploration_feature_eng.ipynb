{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 1. File description \n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["## 0. Import module and setting notebook preferences\n","\n","### 0.1 Import modules"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import src.useful as su\n","import src.labelling as sl\n","import src.image as si\n","import src.model as sm \n","import src.webscraping as sw\n","import src.viz as sv\n","\n","import sqlite3\n","import pandas as pd \n","import numpy as np\n","from scipy.stats import boxcox\n","\n","import nltk \n","from nltk.corpus import stopwords\n","from collections import Counter\n","\n","\n","import plotly.io as pio\n","import plotly.express as px\n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from pandas_profiling import ProfileReport\n","import dtale\n","# from autoviz import data_cleaning_suggestions\n","\n","\n","\n","from sklearn.preprocessing import OneHotEncoder\n","\n","import re\n","import os\n","from pandarallel import pandarallel\n","\n","pandarallel.initialize()\n","sl.Mypandas.initialize()"]},{"cell_type":"markdown","metadata":{},"source":["### 0.2 Notebook preferences"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Setting pandas options\n","pd.options.display.max_columns = None\n","pd.options.display.float_format = '{:,}'.format\n","\n","# Setting plotly as dark\n","pio.templates.default = \"plotly_dark\"\n","plotly_kwargs = {'category_orders': {\"label\": [-1, 0, 1, 3]},\n","                 'color_discrete_sequence': ['#00CC96', '#1f77b4', '#EF553B', '#FECB52']}\n","\n","# Creating SQL database to store all the data for the project\n","database = \"data/main_database.sqlite\"\n","con = sqlite3.connect(database)\n","\n","# Setting path to export viz\n","viz_dir = '/Users/marclamy/Desktop/code/viz/instabot/'"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 0.3 Load and consolidate the data into a single table\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_main = sl.load_main()\n","df_labels = sl.load_labels()\n"," \n","df_main = df_main.merge(df_labels, how='left', on='username')\n","\n","# either fillna, either dropna, depending on the use case, comment/uncomment\n","# df_main['label'] = df_main['label'].fillna(-1).astype(int)\n","# df_main['label'] = df_main['label'].astype(str)\n","df_main['binary_label'] = df_main['label'].apply(lambda x: 1 if x == 3 else x)\n","df_main = df_main.dropna(subset='label').reset_index(drop=True)\n","\n","# df_main = sl.Mypandas(df_main)\n","df_main.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_main[[col for col in df_main.columns if 'label' in col]].info()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["os.system('say \"error\"')\n","# asdf"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 0.4 Counting labels"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["unique_comments = df_main.shape[0]\n","unique_usernames = df_main.drop_duplicates(subset='username').shape[0]\n","\n","labels = df_main['label'].value_counts()\n","labels_unique = df_main.drop_duplicates(subset='username')['label'].value_counts()\n","\n","\n","print(f'{unique_comments = }')\n","print(f'{unique_usernames = }\\n')\n","print(f'Labels for comment \\n{labels}\\n')\n","print(f'Labels unique users \\n{labels_unique}\\n\\n')\n","print(f'% comments labelled: {sum(labels)/unique_comments:.1%}')\n","print(f'% users labelled: {sum(labels_unique)/unique_usernames:.1%}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 1.0 Quick viz - Move this to part 2 for when the data is clean"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 1.X Pandas profiling"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Generate report of the data\n","# profile = ProfileReport(df_main, title=\"Pandas Profiling Report\")\n","# profile.to_file('data/pandas_profiling.html')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_main.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 1.X dtale"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# dtale.show(df_main)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 1.x Autoviz"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# import matplotlib.cm\n","# cmap = matplotlib.cm.get_cmap('Reds')\n","def left_subtract(l1,l2):\n","    lst = []\n","    for i in l1:\n","        if i not in l2:\n","            lst.append(i)\n","    return lst\n","def data_suggestions(data):\n","    \"\"\"\n","    Modified by Ram Seshadri. Original idea for data suggestions module was a Kaggler.\n","    Many thanks to: https://www.kaggle.com/code/itkin16/catboost-on-gpu-baseline\n","    \"\"\"\n","    maxx = []\n","    minn = []\n","    all_cols = list(data)\n","    cat_cols1 = data.select_dtypes(include='object').columns.tolist()\n","    cat_cols2 = data.select_dtypes(include='category').columns.tolist()\n","    cat_cols = list(set(cat_cols1+cat_cols2))\n","    ### The next line may look odd but due to different versions of pandas which\n","    ### treat the definition of float differently, I am forced to use this. Don't change it.\n","    num_cols = data.select_dtypes(include='float16').columns.tolist() + data.select_dtypes(\n","                    include='float32').columns.tolist() + data.select_dtypes(include='float64').columns.tolist()\n","    non_num_cols = left_subtract(all_cols, num_cols)\n","    for i in data.columns:\n","        if i not in cat_cols:\n","            ### for float and integer, no need to calculate this ##\n","            minn.append(0)\n","        else:\n","            minn.append(data[i].value_counts().min())\n","    length = len(data)\n","    nunik = data.nunique()\n","    nulls = data.isna().sum()\n","    df = pd.DataFrame(\n","        {\n","         #'column': list(data),\n","        'Nuniques': nunik,\n","         'NuniquePercent': (100*(nunik/length)),\n","         'dtype': data.dtypes,\n","         'Nulls' : nulls,\n","         'Nullpercent' : 100*(nulls/length),\n","         'Value counts Min':minn\n","        },\n","        columns = ['Nuniques', 'dtype','Nulls','Nullpercent', 'NuniquePercent',\n","                       'Value counts Min'])\n","    newcol = 'Data cleaning improvement suggestions'\n","    print('%s. Complete them before proceeding to ML modeling.' %newcol)\n","    mixed_cols = [col for col in data.columns if len(data[col].apply(type).value_counts()) > 1]\n","    df[newcol] = ''\n","    df['first_comma'] = ''\n","    if len(cat_cols) > 0:\n","        mask0 = df['dtype'] == 'object'\n","        mask1 = df['Value counts Min']/df['Nuniques'] <= 0.05\n","        mask4 = df['dtype'] == 'category'\n","        df.loc[mask0&mask1,newcol] += df.loc[mask0&mask1,'first_comma'] + 'combine rare categories'\n","        df.loc[mask4&mask1,newcol] += df.loc[mask4&mask1,'first_comma'] + 'combine rare categories'\n","        df.loc[mask0&mask1,'first_comma'] = ', '\n","        df.loc[mask4&mask1,'first_comma'] = ', '\n","    mask2 = df['Nulls'] > 0\n","    df.loc[mask2,newcol] += df.loc[mask2,'first_comma'] + 'fill missing'\n","    df.loc[mask2,'first_comma'] = \", \"\n","    mask3 = df['Nuniques'] == 1\n","    df.loc[mask3,newcol] += df.loc[mask3,'first_comma'] + 'invariant values: drop'\n","    df.loc[mask3,'first_comma'] = \", \"\n","    if len(non_num_cols) > 0:\n","        for x in non_num_cols:\n","            if df.loc[x, 'NuniquePercent'] == 100:\n","                df.loc[x, newcol] += df.loc[x,'first_comma'] + 'possible ID column: drop'\n","                df.loc[x,'first_comma'] = \", \"\n","    mask5 = df['Nullpercent'] >= 90\n","    df.loc[mask5,newcol] += df.loc[mask5,'first_comma'] + 'very high nulls percent: drop'\n","    df.loc[mask5,'first_comma'] = \", \"\n","    #### check for infinite values here #####\n","    inf_cols1 = np.array(num_cols)[[(data.loc[(data[col] == np.inf)]).shape[0]>0 for col in num_cols]].tolist()\n","    inf_cols2 = np.array(num_cols)[[(data.loc[(data[col] == -np.inf)]).shape[0]>0 for col in num_cols]].tolist()\n","    inf_cols = list(set(inf_cols1+inf_cols2))\n","    ### Check for infinite values in columns #####\n","    if len(inf_cols) > 0:\n","        for x in inf_cols:\n","            df.loc[x,newcol] += df.loc[x,'first_comma'] + 'infinite values: drop'\n","            df.loc[x,'first_comma'] = \", \"\n","    #### Check for skewed float columns #######\n","    skew_cols1 = np.array(num_cols)[[(np.abs(np.round(data[col].skew(), 1)) > 1\n","                    ) & (np.abs(np.round(data[col].skew(), 1)) <= 5) for col in num_cols]].tolist()\n","    skew_cols2 = np.array(num_cols)[[(np.abs(np.round(data[col].skew(), 1)) > 5) for col in num_cols]].tolist()\n","    skew_cols = list(set(skew_cols1+skew_cols2))\n","    ### Check for skewed values in columns #####\n","    if len(skew_cols1) > 0:\n","        for x in skew_cols1:\n","            df.loc[x,newcol] += df.loc[x,'first_comma'] + 'skewed: cap or drop outliers'\n","            df.loc[x,'first_comma'] = \", \"\n","    if len(skew_cols2) > 0:\n","        for x in skew_cols2:\n","            df.loc[x,newcol] += df.loc[x,'first_comma'] + 'highly skewed: drop outliers or do box-cox transform'\n","            df.loc[x,'first_comma'] = \", \"\n","    ##### Do the same for mixed dtype columns - they must be fixed! ##\n","    if len(mixed_cols) > 0:\n","        for x in mixed_cols:\n","            df.loc[x,newcol] += df.loc[x,'first_comma'] + 'fix mixed data types'\n","            df.loc[x,'first_comma'] = \", \"\n","    df.drop('first_comma', axis=1, inplace=True)\n","    return df\n","###################################################################################\n","def data_cleaning_suggestions(df):\n","    \"\"\"\n","    This is a simple program to give data cleaning and improvement suggestions in class AV.\n","    Make sure you send in a dataframe. Otherwise, this will give an error.\n","    \"\"\"\n","    if isinstance(df, pd.DataFrame):\n","        dfx = data_suggestions(df)\n","        all_rows = dfx.shape[0]\n","        ax = dfx.head(all_rows).style.background_gradient()  \n","        display(ax);\n","    else:\n","        print(\"Input must be a dataframe. Please check input and try again.\")\n","###################################################################################"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["exclude_cols = ['username', 'biography', 'follow_count'] + [col for col in df_main.columns if isinstance(df_main.loc[0, col], (dict, list))]\n","data_cleaning_suggestions(df_main.drop(exclude_cols, axis=1))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["os.system('say \"auto viz done\"')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 1.0 Data cleaning and Univariate Analysis"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["For the following: \n","* Clean the column \n","    * Remove missing value/add a missing flag column\n","    * Flag and remove outliers\n","* Add new features\n","* Data viz\n","    * Look at the distribution for each label\n","    * Look at describe per label \n","    * Next to the title, add - Done / - missing markdown\n","---\n","\n","\n","Sub groups of 1.0 with: \n","* Biography \n","    * bio/bio wo emoji \n","    * bio emoji count \n","* Comment \n","    * Comment/wo emoji\n","    * Comment int cols \n","* Follow/er count \n","* Domain\n","* Post/video count \n","    * and highlight_reel_count\n","* Binary\n","    * hide like and view count \n","    * has guides \n","    * has clips \n","    * is private \n","    * is embeds \n","    * is joined \n","    * is verified\n","* Professional/business account \n","* Posts list\n","* Dates diff \n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 1.1 Comments - Done"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 1.1.0 Gram functions\n","\n","Creating functions to clean/calculate/show different grams of the comments. Will be reusing them for biography."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def clean_and_tokenize(biographies):\n","    \"\"\"\n","    Given a list of biographies, this function performs the following steps:\n","    1. Remove unwanted characters from each biography\n","    2. Tokenize the cleaned text into words\n","    3. Remove stop words from the tokenized text\n","    4. Return the filtered list of tokens\n","\n","    Parameters:\n","    biographies (list): List of strings representing the biographies\n","\n","    Returns:\n","    filtered (list): List of lists of filtered tokens\n","    \"\"\"\n","    # Remove unwanted characters\n","    cleaned = [re.sub(r'[^\\w\\s\\d]', '', b).lower() for b in biographies]\n","    # Tokenize the text\n","    tokenized = [nltk.tokenize.word_tokenize(b) for b in cleaned]\n","    # Remove stop words\n","    stop_words = set(stopwords.words(\"english\"))\n","    filtered = [[w for w in t if w.lower() not in stop_words or w.isdigit()] for t in tokenized]\n","    return filtered\n","    \n","\n","def most_frequent_grams(tokens, grams):\n","    bigrams = [', '.join(b) for t in tokens for b in list(nltk.ngrams(t, grams))]\n","    most_frequent = dict(Counter(bigrams))\n","    return most_frequent\n","\n","\n","def calculate_grams(df, column, max_gram_degree):\n","    df_grams = pd.DataFrame(columns=['gram', 'count', 'label', 'gram_degree'])\n","    for label in (0, 1, 2, 3):\n","        tokens = clean_and_tokenize(df.query(f'label == {label}')[column])\n","        for gram in (1, 2, 3, 4):\n","            top_tokens = most_frequent_grams(tokens, gram)\n","            # top_tokens = {i:[j] for i, j in top_tokens.items()}\n","            # df_top_tokens = pd.DataFrame(top_tokens, columns=[f'label_{label}_gram_{gram}', f'label_{label}_count_{gram}'])\n","            current_df = pd.DataFrame(top_tokens.items(), columns=['gram', 'count'])\n","            current_df['gram_degree'] = gram\n","            current_df['label'] = label\n","            df_grams = pd.concat([df_grams, current_df], axis=0)\n","    return df_grams\n","\n","\n","def keep_top_n_grams(df, n):\n","    df_grams_grouped = df.query('label in (0, 1, 3)').sort_values(by=['label', 'gram_degree', 'count'], ascending=[True, True, False])\n","    df_grams_grouped['row_number'] = df_grams_grouped.groupby(['label', 'gram_degree']).cumcount() + 1\n","    grams_to_keep = df_grams_grouped.query('row_number <= @n')['gram'].unique().tolist()\n","    df = df.query('gram in @grams_to_keep').sort_values(['label', 'gram_degree', 'count'], ascending=[True, True, False])\n","    return df\n","\n","\n","\n","def plot_grams(df): \n","    for degree in df.gram_degree.unique(): \n","        fig = px.bar(\n","            df.query('gram_degree == @degree'), \n","            x='gram', \n","            y='count', \n","            color='label', \n","            # barmode='group', \n","            **plotly_kwargs,\n","            title=f'Most grams by label and degree {degree}')\n","        fig.update_xaxes(tickangle=-90)\n","        # fig.update_layout(xaxis={'categoryorder':'total descending'})\n","        \n","        display(fig)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 1.1.1 Wordcloud"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot wordcloud\n","for label in (0, 1, 3):\n","    print(f'Label: {label}')\n","    wordcloud = WordCloud(width=1800, \n","                          height=1200, \n","                          min_font_size=1, \n","                          max_words=500, \n","                          colormap='tab10')\n","                          \n","    word_list = ' '.join([word for list in df_main.query(f'label == {label}')['comment_wo_emoji'].str.split(' ') for word in list])\n","    wordcloud.generate(word_list)\n","\n","    plt.figure()\n","    plt.imshow(wordcloud, interpolation=\"bilinear\")\n","    plt.axis(\"off\")\n","    plt.savefig(f'{viz_dir}/wordcloud_comment_{label}.png', dpi=300, pad_inches=0.0, bbox_inches='tight')\n","    plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 1.1.2 Calculating bi/tri/quad grams"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sub_df = df_main.loc[:, ['comment_wo_emoji', 'label']].drop_duplicates()\n","df_bio_grams = calculate_grams(sub_df, 'comment_wo_emoji', 20)\n","df_bio_grams = keep_top_n_grams(df_bio_grams, 30)\n","plot_grams(df_bio_grams)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 1.1.3 Feature engineering\n","For each comment, count how many of the top 30 grams to they have"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Adding tri/quad grams count to main df\n","bots_triquadgrams = df_bio_grams.query('label > 0 and gram_degree > 2').gram.unique().tolist()\n","\n","def count_grams(text, grams):\n","    count = 0\n","    for gram in grams:\n","        gram = gram.split(',')\n","        if all(word in text for word in gram):\n","            count += 1\n","    return count\n","\n","\n","df_main['comment_grams_count'] = df_main['comment_wo_emoji'].apply(lambda x: count_grams(x, bots_triquadgrams))\n","\n","print('% of each label having at least one tri/quad gram')\n","for label in (0, 1, 3): \n","    current_df = df_main.query(f'label == {label}')['comment_grams_count'] > 0\n","    current_df = current_df.value_counts(normalize=True)\n","    display(current_df.to_frame(name=f'Label: {label}').round(3))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Adding comment length and word count\n","df_main['comment_length'] = df_main['comment'].str.len()\n","df_main['comment_word_count'] = df_main['comment'].str.count(' ') + 1\n","\n","# Dropping columns\n","df_main = df_main.drop(columns=['comment', 'comment_wo_emoji', 'comment_emoji'])\n","df_main.head(1)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 1.2 Comment likes and time difference - 90% done, missing the insights of last chart\n","\n","There are multiple other columns about the comments of the users. \n","* `comments_likes`: the number of likes received by the bots. I suspect them to be inflated by many as found some bots with more than 200 likes seconds after the post was posted. This data is mainly produced by bots (they like themselves and other bots like their comment) but legit users like you and me could also like the comment. \n","* `comment_time_difference`: number of seconds between post being publish and the comment of the bots"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cols = ['comments_likes', 'comment_time_difference']\n","\n","print('Missing values: ')\n","df_main[cols].isna().sum()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 1.2.1 Comments Likes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_main.describe_column_by_colcat('comments_likes')[0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Calculating outliers and flagging them\n","lower, upper = df_main.outliers_bound('comments_likes')\n","df_main['outlier'] = df_main['comments_likes'].apply(lambda x: '' if lower < x < upper else 'comments_likes')\n","lower, upper"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Biography length distribution\n","fig = px.histogram(\n","    df_main.query('outlier == \"\"'),\n","    x='comments_likes', \n","    nbins=200,\n","    color=\"label\",\n","    opacity=.7,\n","    marginal=\"box\", # or violin, rug)\n","    title=f'Distribution of Biography Length per Label for users with biographies',\n","    **plotly_kwargs\n","    )\n","\n","fig = fig.update_layout(barmode='overlay')\n","\n","# sv.save_plotly_fig(fig)\n","sv.save_plotly_fig(fig)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["(df_main.query('label == 3')['comments_likes'] > 0).value_counts(normalize=True).multiply(100).round(1)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["There's a very clear distinction between the distribution of the likes on the comments. \n","* Other bots have almost no likes on their comments. Their Q3 is at 0 and avg at 11 with only 10% of them having at least a like.\n","* The bots definitely have a weird distribution. It's kinda like waves and they don't have anything between 300 and 500 but then have a bump, this is sus.\n","* Legit users have what seems to be a very skewed but guessable distribution. The more the likes, the less the people have them.\n","\n","It'll be interesting to see how that that correlates with time difference"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 1.2.1 Comment Time Difference"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_main.describe_column_by_colcat('comment_time_difference')[0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Calculating outliers and flagging them\n","lower, upper = df_main.outliers_bound('comment_time_difference')\n","df_main['outlier'] = df_main['comment_time_difference'].apply(lambda x: '' if lower < x < upper else 'comment_time_difference')\n","lower, upper"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Biography length distribution\n","fig = px.histogram(\n","    df_main.query('outlier == \"\"'),\n","    x='comment_time_difference', \n","    nbins=200,\n","    color=\"label\",\n","    opacity=.7,\n","    marginal=\"box\", # or violin, rug\n","    title=f'Distribution of Comment Time Difference per Label<br><sub>Comment time difference is the time difference between the comment and the post in seconds',\n","    **plotly_kwargs\n","    )\n","\n","fig = fig.update_layout(barmode='overlay')\n","fig  = fig.add_vline(x=0, line_color=\"green\", line_width=2, annotation_text=\"0\")\n","\n","# sv.save_plotly_fig(fig)\n","sv.save_plotly_fig(fig)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["(df_main.query('label == 3')['comments_likes'] > 0).value_counts(normalize=True).multiply(100).round(1)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Same as before, there is a clear distinction between the three distribution: \n","* Bots are way more skewed towards 0 with most commenting under 70seconds\n","* legit users "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_main[[col for col in df_main if 'com' in col and 'post' not in col]].isna().sum()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 1.2 Biography - 90% done"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(f'Nulls: {df_main.biography.isna().sum()}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_main.biography.describe(include='all')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 1.2.1 biography wordcloud"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot wordcloud\n","for label in (0, 1, 3):\n","    print(f'Label: {label}')\n","    wordcloud = WordCloud(width=1800, \n","                          height=1200, \n","                          min_font_size=1, \n","                          max_words=500, \n","                          colormap='tab10')\n","                          \n","    word_list = ' '.join([word for list in df_main.query(f'label == {label}')['biography_wo_emoji'].str.split(' ') for word in list])\n","    wordcloud.generate(word_list)\n","\n","    plt.figure()\n","    plt.imshow(wordcloud, interpolation=\"bilinear\")\n","    plt.axis(\"off\")\n","    plt.savefig(f'{viz_dir}/wordcloud_biography_{label}.png', dpi=300, pad_inches=0.0, bbox_inches='tight')\n","    plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 1.2.2 Calculating bi/tri/quad grams"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sub_df = df_main.loc[:, ['biography_wo_emoji', 'label']].drop_duplicates()\n","df_bio_grams = calculate_grams(sub_df, 'biography_wo_emoji', 20)\n","df_bio_grams = keep_top_n_grams(df_bio_grams, 30)\n","plot_grams(df_bio_grams)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 1.2.3 Feature engineering\n","For each biography, count how many of the top30 grams to they have"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Adding tri/quad grams count to main df\n","bots_triquadgrams = df_bio_grams.query('label > 0 and gram_degree > 2').gram.unique().tolist()\n","\n","def count_grams(text, grams):\n","    count = 0\n","    for gram in grams:\n","        gram = gram.split(',')\n","        if all(word in text for word in gram):\n","            count += 1\n","    return count\n","\n","\n","df_main['biography_grams_count'] = df_main.biography_wo_emoji.apply(lambda x: count_grams(x, bots_triquadgrams))\n","\n","print('% of each label having at least one tri/quad gram')\n","for label in (0, 1, 3): \n","    current_df = df_main.query(f'label == {label}')['biography_grams_count'] > 0\n","    current_df = current_df.value_counts(normalize=True)\n","    display(current_df.to_frame(name=f'Label: {label}').round(3))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Adding comment length and word count\n","df_main['biography_length'] = df_main['biography'].str.len()\n","df_main['biography_word_count'] = df_main['biography'].str.count(' ') + 1\n","df_main['biography_linebreak_count'] = df_main['biography'].str.count('\\n')\n","\n","# Dropping columns\n","df_main = df_main.drop(columns=['biography', 'biography_wo_emoji', 'biography_emoji'])\n","df_main.head(1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Biography length distribution\n","sub_df = df_main.query('biography_length > 0')\n","fig = px.histogram(\n","    sub_df,\n","    x='biography_length', \n","    nbins=50,\n","    color=\"label\",\n","    opacity=.7,\n","    marginal=\"box\", # or violin, rug)\n","    title=f'Distribution of Biography Length per Label for users with biographies',\n","    **plotly_kwargs\n","    )\n","\n","fig = fig.update_layout(barmode='overlay')\n","\n","# sv.save_plotly_fig(fig)\n","sv.save_plotly_fig(fig)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["For the users having a biography:\n","* Legit users tend to have a shorter biography with 50% less than 48 characters. Their IQR is 77 and they occupy all bounds of the distribution. \n","* Bots have half of their bio between 55 nd 81 characters (IQR) and generally dont use less than 20 characters or more than 120 \n","* The other type of bot, also has its own distribution with 75% of their bio having more than 120 characters. "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 1.3 Follow & Follower Count"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 1.3.1 Follow Count"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_main.describe_column_by_colcat('follow_count')[0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Calculating outliers and flagging them\n","lower, upper = df_main.outliers_bound('follower_count')\n","df_main['outlier'] = df_main['follower_count'].apply(lambda x: '' if lower < x < upper else 'follower_count')\n","lower, upper"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for label in (0, 1, 3):\n","    print(f'Label: {label}')\n","    display(df_main.query(f'label == {label}')['outlier'].value_counts(normalize=True).multiply(100).round(1).to_frame())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Describing column for each label\n","sub_df, fig = df_main.describe_column_by_colcat('follow_count')\n","\n","print('Follow count: ')\n","display(sub_df.style.format('{:,}'))\n","sv.save_plotly_fig(fig)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig = px.histogram(df_main.query('outlier.str.contains(\"follow_count\") == False'),\n","                   x=\"follow_count\", \n","                   nbins=500,\n","                   color=\"label\",\n","                   opacity=.7,\n","                   marginal=\"box\", # or violin, rug)\n","                   title='Distribution of the follow count per label',\n","                   **plotly_kwargs\n","                   )\n","\n","\n","fig = fig.update_layout(barmode='overlay')\n","\n","# sv.save_plotly_fig(fig)\n","sv.save_plotly_fig(fig)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import sketch\n","df_main.sketch.ask('which columns are integers?')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The distribution of the follow count is very interesting. All labels' interquartile range (between Q1 and Q3) occupy the 0-2.5k follow count range. the main category of bots (sex bots, main obj of the project) tend to follow between 0 and 243 accounts. from 240 to 1,030 there are the legit users and from 1100 to 2.2k, the other type of bot. \n","\n","\n","The bots (1) tend to be more about mass commenting, that's how they scam people\n","The legit user have a larger distribution and are more diverse\n","The other type of bot have a higher distribution."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 1.x Follower count"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Calculating outliers and flagging them\n","lower, upper = df_main.outliers_bound('follower_count')\n","df_main['outlier'] = df_main['comment_time_difference'].apply(lambda x: '' if lower < x < upper else 'comment_time_difference')\n","lower, upper"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The upper bound is kinda low. It's not really hard to reach 2.5k followers on IG."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Describing column for each label\n","sub_df, fig = df_main.describe_column_by_colcat('follower_count')\n","\n","print('Follower count: ')\n","display(sub_df.style.format('{:,}'))\n","sv.save_plotly_fig(fig)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Seeing the bots at 6k max follower and other bots at 14k, I can assume that none, even if not labelled, can have more than 30k followers"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_main.query('label > 0').sort_values('follower_count', ascending=False).head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["follower_count_q95 = df_main.follower_count.quantile(.95)\n","follower_count_q90 = df_main.follower_count.quantile(.90)\n","\n","\n","fig = px.histogram(df_main.query('follower_count < 30000'), \n","                   x=\"follower_count\", \n","                   nbins=200,\n","                   color=\"label\",\n","                   opacity=.7,\n","                   marginal=\"box\", # or violin, rug\n","                   title='Distribution of the follower count per label.<br><sub>Numbers next to lines are the quantiles.',\n","                   **plotly_kwargs\n","                #    color_discrete_sequence=px.colors.qualitative.Plotly[3:]\n","                   )\n","\n","max_follower_bot = df_main.query('label > 0')['follower_count'].max() * 2\n","\n","fig = fig.update_layout(barmode='overlay', xaxis_range=(0, max_follower_bot))\n","\n","sv.save_plotly_fig(fig)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig = px.histogram(df_main.query('follower_count < 25000'), \n","                   x=\"follower_count\", \n","                   nbins=2000,\n","                   color=\"label\",\n","                   opacity=.7,\n","                   marginal=\"box\", # or violin, rug\n","                   title='Distribution of the follower count per label.',\n","                   **plotly_kwargs\n","                   )\n","\n","\n","fig = fig.update_layout(barmode='overlay', \n","\n","                        )\n","sv.save_plotly_fig(fig)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 1.x Posts count"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Same as for the follow_count, there is a difference in the distribution of the follower count. It varies less, with the upper fense of the bots in the IQR of the legit users, and lower than the IQR of the other bots. Very skewed distribution "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Describing column for each label\n","sub_df, fig = df_main.describe_column_by_colcat('post_count')\n","\n","print('Posts count: ')\n","display(sub_df.style.format('{:,}'))\n","sv.save_plotly_fig(fig)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# df_main['post_count']\n","fig = px.histogram(df_main.query('post_count < 5000'), \n","                   x=\"post_count\", \n","                   nbins=5000,\n","                   color=\"label\",\n","                   opacity=.8,\n","                   marginal=\"box\", # or violin, rug\n","                   title='Distribution of the follower count per label.',\n","                   **plotly_kwargs\n","                   )\n","\n","\n","fig = fig.update_layout(barmode='overlay', xaxis_range=(0, 100))\n","\n","sv.save_plotly_fig(fig)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 1.x Comment time difference FIND ALL THE OUTLIERS FOR EACH COLUMNS AND ADD A OUTLIER COLUMN FOR THE USERS"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Describing column for each label\n","sub_df, fig = df_main.describe_column_by_colcat('comment_time_difference')\n","\n","print('Comment time diff: ')\n","display(sub_df.style.format('{:,}'))\n","sv.save_plotly_fig(fig)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# df_main['post_count']\n","fig = px.histogram(df_main,\n","                   x=\"comment_time_difference\", \n","                   nbins=7000,\n","                   color=\"label\",\n","                   marginal=\"box\", # or violin, rug\n","                   title='Distribution of the follower count per label.',\n","                   **plotly_kwargs\n","                   )\n","\n","\n","fig = fig.update_layout(barmode='overlay')\n","\n","sv.save_plotly_fig(fig)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 1.x Comment time difference WHATS THE AVERAG DIFFERENCE BETWEEN EACH POST? COULD BE UPLOADED SUPER FAST"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def round_or_int(val, **kwargs): \n","    if pd.isna(val): \n","        return val\n","    elif val == int(val): \n","        return int(val) \n","    else:\n","        return round(val, ndigits=kwargs['ndigits'])\n","\n","def calculate_posts_time_diff_btw_each(date_concat): \n","    if pd.isna(date_concat): \n","        return date_concat\n","    dates = [pd.to_datetime(date) for date in date_concat.split(',')]\n","    dates = sorted(list(set(dates)))[:12]\n","    dates_diff = pd.Series(dates).diff().dt.seconds.dropna().reset_index(drop=True)\n","    dates_diff_summary = dates_diff.describe().apply(round_or_int, ndigits=2).to_dict()\n","    dates_diff_summary.update({'dates_diff': dates_diff.tolist(),\n","                               'unique_values': len(set(dates_diff)),\n","                               'total_values': len(dates_diff)\n","                               })\n","\n","    # display(pd.DataFrame({i:[j] for i, j in dates_diff_summary.items()}))\n","    dates_diff_summary = {'dates_diff_' + i:j for i, j in dates_diff_summary.items()}\n","    return dates_diff_summary\n","\n","df_main['posts_diff_btw_each_summary_seconds'] = df_main['posts_concat_posted_time'].parallel_apply(calculate_posts_time_diff_btw_each)\n","# calculate_posts_time_diff_btw_each(df_main.loc[1, 'posts_concat_posted_time'])\n","df_posts_diff_btw_each_summary = pd.json_normalize(df_main['posts_diff_btw_each_summary_seconds'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_main = pd.concat([df_main, df_posts_diff_btw_each_summary], axis=1)\n","df_main.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# sub_df = df_main[['dates_diff_follows_pattern', 'post_count', 'label']]\n","\n","# fig = px.scatter_matrix(sub_df, color='label', title='Pairplot')\n","# sv.save_plotly_fig(fig)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # Describing column for each label\n","# sub_df, fig = df_main.describe_column_by_label('dates_diff_50%')\n","\n","# print('dates_diff_50%: ')\n","# display(sub_df.style.format('{:,}'))\n","# fig"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 1.x df_main.describe_column_by_label"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# df_main.describe_column_by_label"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 1.1 Comments"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # lowering everything \n","# for column in df_main.select_dtypes('object').columns: \n","#     if 'comment' in column: \n","#         df_main[column] = df_main[column].str.lower()\n","#         print(f'Lowered {column}')\n","\n","\n","# # Adding comment length and word count\n","# df_main['comment_length'] = df_main['comment'].str.len()\n","# df_main['comment_word_count'] = df_main['comment'].str.count(' ') + 1\n","\n","\n","# # Adding flag for specific wordlike sex only, exclusive, etc\n","\n","# df_main.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["all_words = [word for list in df_main['biography'].str.split(' ') for word in list]\n","print(f'Total words: {len(all_words):,}')\n","print(f'Unique words: {len(set(all_words)):,}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # Plot wordcloud\n","# for num in (0,1,3):\n","#     wordcloud = WordCloud(width=1800, \n","#                           height=1200, \n","#                           min_font_size=1, \n","#                           max_words=500, \n","#                           colormap='tab10')\n","                          \n","#     word_list = ' '.join([word for list in df_main.query(f'label == {num}')['comment_wo_emoji'].str.split(' ') for word in list])\n","#     wordcloud.generate(word_list)\n","#     plt.figure()\n","#     plt.imshow(wordcloud, interpolation=\"bilinear\")\n","#     plt.axis(\"off\")\n","#     plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#### HERE MAKE A VLAUE COUNT FOR EACH LABEL"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["* Both legit and other bots have the word \"first\", but not for the same reason. For the legits, its only to be first to comment where for the bots it's the first who read out to them. Let's check the average sentence length where first is in. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# for num in (0, 1, 3):\n","#     mean = df_main.query(f'comment.str.contains(\"first\") and label == {num}')['comment_word_count'].mean()\n","#     print(f'Mean for {num}: {round(mean)}')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 1.comment.grams"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# df_main"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sub_df = df_main.loc[:, ['comment_wo_emoji', 'label']].drop_duplicates()\n","df_bio_grams = calculate_grams(sub_df, 'comment_wo_emoji', 4)\n","df_bio_grams = keep_top_n_grams(df_bio_grams, 30)\n","plot_grams(df_bio_grams)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 1.x Business/Verified\n","\n","There are multiple columns like: `is_business_account`, `is_professional_account`, `category_enum`, `category_name`, `business_category_name`, `business_contact_method`"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["business_columns = ['is_business_account', 'is_professional_account', 'category_enum', 'category_name', 'business_category_name', 'business_contact_method']\n","\n","sub_df = df_main[business_columns + ['label']]\n","\n","for col in business_columns: \n","    col_na = sub_df[col].isna()\n","    if col_na.sum() != 0:\n","        sub_df[col] = sub_df[col].isna().astype(int)\n","\n","sub_df.groupby(business_columns, as_index=False)['label'].count()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_business_groupby = df_main.groupby(['is_professional_account', 'is_business_account', 'label'], as_index=False)['username'].count()\n","df_business_groupby['count_%'] = df_business_groupby['username'] / df_business_groupby['username'].sum()\n","\n","df_business_groupby.style.background_gradient()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Many bots are creating professional or business accounts. Seems like if you're a business, you're a professional but if you're pro, you're not necessarily a business. The highest combination is that most legit users and bots don't have a professional account. It's good to note that for the otherbots, there are as more accounts as professionals/business than normal accounts."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["px.sunburst(df_main.dropna(subset='category_enum').fillna('na'),\n","            path=['category_enum', 'category_name'],\n","            color='label',\n","            ).update_layout(uniformtext=dict(minsize=10, mode='hide'))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# asdf"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig = px.histogram(df_main, \n","                    x='category_name', \n","                    color='label',\n","                    title=f'Distribution of labels for each category of category_name for bots and legit users')\n","\n","fig = fig.update_xaxes(tickangle=-90, categoryorder='total descending') \n","fig"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig = px.histogram(df_main.query('label > 0'), \n","                    x='category_name', \n","                    color='label',\n","                    title=f'Distribution of labels for each category of category_name for bots only')\n","\n","fig = fig.update_xaxes(tickangle=-90, categoryorder='total descending') \n","fig"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig = px.histogram(df_main.query('label > 0'), \n","                    x='business_category_name', \n","                    color='label',\n","                    title=f'Distribution of labels for each category of business_category_name for bots and legit users')\n","\n","fig = fig.update_xaxes(tickangle=-90, categoryorder='total descending') \n","fig"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["It's interesting to see the difference between the bots and the sex bots. Even though many have been miss labelled (they'll both be marked at 1 when developping the model) bet ween bot or otherbot, there is a significative distinction between how they identify themselves.\n","\n","Other bots seems to be more entreupreuners, financial service or investment firm where bots are more video creator (you bet), personnal blog or gamer."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_main.query('label > 0')['category_name'].value_counts().sort_values(ascending=True)[df_main.query('label > 0')['category_name'].value_counts().sort_values(ascending=True).lt(12)].index"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Keeping only binary columns\n","df_main = df_main.drop(['category_name', 'business_category_name', 'business_contact_method'], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_main"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### IV Changing values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Pronouns"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sub_df = df_main.copy(deep=True)\n","sub_df['pronoun_null'] = sub_df['pronouns'].isna().astype(int)\n","\n","sub_df.groupby(['pronoun_null', 'label'], as_index=False)['username'].count() "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["There are still 11 bots that have a pronoun. Which do they identify to you ask??"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_main.pronouns.value_counts().head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pronouns = {\"he\": 'male', \"him\": 'male', \"his\": 'male', \"himself\": 'male', \"she\": 'female', \"her\": 'female', \"hers\": 'female', \"herself\": 'female'}\n","\n","def find_gender(pronoun): \n","    if pd.notna(pronoun):\n","        first_pronoun = re.split(',| ', pronoun)[0]\n","        try:\n","            return pronouns[first_pronoun]\n","        except KeyError: \n","            return 'nonbinary'\n","\n","df_main['pronouns'].apply(find_gender).value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_main = df_main.drop('pronouns', axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### POst count and video count"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Next"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Comment likes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def outliers_bound(self, col):\n","    q1, q3 = self[col].quantile([.25, .75])\n","    iqr = q3 - q1\n","    lower_bound = q1 - (1.5 * iqr)\n","    upper_bound = q3 + (1.5 * iqr)\n","    return lower_bound, upper_bound\n","\n","def find_outliers(self, col, bounds=[]):\n","    '''Given a columns, return a pd.Series with True/False for each value'''\n","    if len(bounds) == 2: \n","        lower_bound, upper_bound = bounds\n","    else: \n","        lower_bound, upper_bound = outliers_bound(self, col)\n","\n","    return self[col].fillna(self[col].quantile(.5)).apply(lambda x: not lower_bound <= x <= upper_bound)\n","\n","\n","def find_all_outliers(self, dtypes=None, columns=None, exclude=[]):\n","    # Add or remove column to the list\n","    if dtypes != None: \n","        columns = list(self.select_dtypes(dtypes))\n","    elif isinstance(columns, str):\n","        columns = [columns]\n","    for col in exclude: \n","        columns.remove(col)\n","\n","    # Find outliers and concat\n","    base_series = pd.Series(['']*self.shape[0])\n","    for index, col in enumerate(columns): \n","        col_outliers = find_outliers(self, col).map({True: col, False: ''})\n","        base_series = base_series.str.cat(col_outliers, sep=' ').str.strip()\n","        \n","    return base_series\n","\n","\n","find_outliers(df_main, 'comments_likes')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_main.outliers_bound('comments_likes')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for x in df_main.describe_column_by_colcat('comments_likes'): \n","    display(x)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# def better_describe(self, column): \n","#     data = {}\n","#     # data = {'count': [self[column].shape[0]],\n","#     #         'sum': [self[column].sum()],\n","#     #         }\n","\n","#     # data.update({'min': [self[column].min()]})\n","#     for q in range(1, 100): \n","#         data.update({f'{q}%': [self[column].quantile(q/100)]})\n","#     # data.update({'max': [self[column].max()]})\n","    \n","\n","#     df = pd.DataFrame(data).T#.drop_duplicates(0, keep='first')\n","#     df['diff'] = df.iloc[::-1].diff()\n","#     df['var'] = [np.nan] + (df['diff'] / df[0] * -1).round(2).tolist()[:-1]\n","\n","#     return df.query('var >= .1').round(2)\n","\n","\n","# better_describe(df_main, 'comments_likes').head(50)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["* different quantile\n","* skewness\n","* inf values\n","* unique values\n","* null values\n","* sum\n","* quantile \n","* count\n","* Min \n","* max\n","* std\n","*"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["There is no doubts on that, bots comment the faster and it's their main strategy which makes sense, the sonner the comment the more reach it has. \n","\n","There is a clear distinction between the bots and the other bots. The bots are way more in volume "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_main.query('label == 3')['comments_likes']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig = px.box(df_main, \n","             x='label', \n","             y='comments_likes',\n","             color='label',\n","             **plotly_kwargs\n","             )\n","\n","fig.update_layout(yaxis_range=(0, 600))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# df = px.data.iris()\n","# fig = px.scatter(df, x=\"sepal_width\", y=\"sepal_length\", color=\"species\", marginal_y=\"violin\",\n","#            marginal_x=\"box\", trendline=\"ols\", template=\"simple_white\")\n","# fig.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 1.x Comment time diff"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_main"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig = px.violin(df_main, \n","                x='label', \n","                y='comment_time_difference',\n","                color='label',\n","                box=True, \n","                # kde=1,\n","                # meanline_visible=True,\n","                # points='all',\n","                **plotly_kwargs\n","                )\n","\n","fig.update_layout(yaxis_range=(0, 450))\n","fig.update_traces(bandwidth=100, selector=dict(type='violin'))\n","\n","fig"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["There's a clear distinction between the different labels: legit users tend to comment, even after some time where the bots are way faster. They comment only\n","\n","The collected data is not representative of the overall population but I think that by collecting more, I would have just extended the range of the `0` class."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Investigate"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# adsf"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["column = 'comments_likes'\n","cols_co_keep = [column] + ['username', 'biography', 'label', 'labelling_technique', 'domain', 'comment']\n","\n","sub_df = df_main.query('label > 0').sort_values(column, ascending=False)[cols_co_keep].drop_duplicates()\n","# display(df_main.head(50))\n","username = 'resolute-tan-oxpecker'\n","# print(sl.load_table('select '))\n","# print(username)\n","\n","display(sl.load_labels(include_all=True).query('username == @username'))\n","# display(sl.load_table('username_mapping').query('cooler_name == @username'))\n","# df_main.query('username == @username')\n","\n","\n","sub_df.query('comments_likes > 1000 and label > 0')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["asdf"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Done investigating"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 2.0 Multi variate analysis - Past this point the data is entirely clean and ready for ML."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# asdf"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Removing columns for now as there are way too many and feel overwhelmed\n","cols_to_remove = ['posts_sum', 'posts_avg', 'posts_max', 'posts_min', 'posts_con', 'dates_dif']\n","cols_to_remove = [col for col in df_main.columns if any([x in col for x in cols_to_remove])] + ['biography', 'biography_wo_emoji', 'biography_emoji', 'domain', 'comment', 'comment_wo_emoji',\n","                                                                                                'comment_emoji', 'labelling_technique', 'posts_diff_btw_each_summary_seconds']\n","cols_to_keep = [col for col in df_main.columns if col not in cols_to_remove]\n","\n","df_main = df_main[cols_to_keep]\n","df_main.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_main"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 2.1 Pair plot (remove outliers)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_main.columns"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","# cols = ['follow_count', 'follower_count', 'post_count', 'video_count', 'is_private', 'binary_label', 'posts_hours_diff']\n","# sns.pairplot(df_main[cols], \n","#              hue=\"binary_label\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 2.2 Follow count & follow "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# df_main.query('comment_time_difference > 74 and label > 0').sort_values('comment_time_difference', ascending=False)[['username', \n","#                                                                                                         'post_count', \n","#                                                                                                         'comment_time_difference', \n","#                                                                                                         'biography', \n","#                                                                                                         'label', \n","#                                                                                                         'labelling_technique']].head(40)\n","\n","px.scatter_3d(df_main,\n","              x='follow_count',\n","              y='follower_count',\n","              z='label',\n","              color='label')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 1.1.6 Pairplot"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["asdfasdfsafasd"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### 1.1.7"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 2.x Comment likes vs comment time diff"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sub_df = df_main.copy(deep=True)\n","\n","\n","# sub_df[]\n","\n","fig = px.scatter(sub_df,\n","           x='comment_time_difference', \n","           y='comments_likes',\n","           color='label')\n","# you can be casual and nice to read but remember that the public is from an age of 21 to 100 years old. \n","fig.update_layout()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### ?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["percent99 = df_main['follower_count'].quantile(0.9)\n","sub_df = df_main.query('follower_count < @percent99')\n","\n","\n","fig = px.histogram(sub_df, \n","                   x=\"follower_count\", \n","                   nbins=100,\n","                   color=\"label\",\n","                   marginal=\"box\", # or violin, rug\n","                   )\n","\n","fig.update_layout(barmode='overlay')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import plotly.figure_factory as ff\n","import numpy as np\n","\n","sub_df = df_main.query('follow_count.notna()')\n","\n","\n","labels = ['0', '1', '3']\n","hist_data = [sub_df[sub_df['label'] == int(x)]['follow_count'] for x  in labels]\n","\n","fig = ff.create_distplot(hist_data, labels,\n","                         bin_size=[200, 200, 200], show_curve=True)\n","\n","# Add title\n","fig.update(layout_title_text='Hist and Rug Plot')\n","fig.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Export df_ml"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# asdlf"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_ml = df_main.copy(deep=True)\n","\n","# removing na on label,\n","df_ml = df_ml.dropna(subset='label')\n","\n","# df_ml['biography'] = df_ml['biography'].fillna('')\n","# df_ml['domain'] = df_ml['domain'].fillna('')\n","# df_ml['pronouns'] = df_ml['pronouns'].fillna('')\n","\n","\n","# removing agg columns \n","df_ml['posts_na_flag'] = df_ml['posts_days_diff'].isna().astype(int)\n","for col in df_ml.columns: \n","    if (col[:6] == 'posts_' and 'concat' not in col and 'posted_time' not in col) or 'dates_diff' in col: \n","        try:\n","            df_ml[col] = df_ml[col].fillna(df_ml[col].mean())\n","        except: \n","            df_ml = df_ml.drop(col, axis=1)\n","    elif 'concat ' in col or 'posted_time' in col: \n","        df_ml = df_ml.drop(col, axis=1)\n","\n","# Adding comment feature \n","# df_ml['comment_length'] = df_ml['comment'].str.len()\n","\n","# removing na on follow count\n","df_ml = df_ml.dropna(subset='follow_count')\n","# df_ml['biography'] = df_ml['biography'].apply(lambda x: 1 if len(x) > 0 else 0)\n","# df_ml['domain'] = df_ml['domain'].apply(lambda x: 1 if len(x) > 0 else 0)\n","# df_ml['pronouns'] = df_ml['pronouns'].apply(lambda x: 1 if len(x) > 0 else 0)\n","df_ml['label'] = df_ml['label'].astype(float)\n","for col in ['pronouns', 'domain', 'business_contact_method', 'business_category_name', 'category_name', 'category_enum', 'posts_concat_likes', 'posts_concat_is_video', 'posts_concat_video_views', 'posts_concat_comments', 'posts_concat_tagg_count', 'labelling_technique', 'outlier']: \n","    df_ml = df_ml.drop(col, axis=1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_ml['label'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_ml.isna().sum().tail(40)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["s = df_ml.isna().sum().head(50).eq(0)\n","cols = s[s].index\n","cols"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_ml.to_sql('ml', con, if_exists='replace', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# # DUMB ML TEST \n","# df_dumb_ml = df_ml.copy(deep=True)\n","\n","\n","# for col in df_ml.columns: \n","#     # print(len(df_ml[col].unique()))\n","#     unique_vals = df_dumb_ml[col].unique()\n","#     if 2 < len(unique_vals) < 10000:\n","#         for x in df_dumb_ml[col].describe().iloc[1:-1]: \n","#             df_dumb_ml[col + str(x)] = df_dumb_ml[col].ge(x)\n","        \n","#         df_dumb_ml.drop(col, axis=1)\n","\n","# df_dumb_ml.to_sql('dumb_ml', con, if_exists='replace', index=False)\n","# df_dumb_ml.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_ml.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Quick ML\n","from sklearn.tree import DecisionTreeClassifier, plot_tree\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import *\n","from xgboost import XGBClassifier, DMatrix\n","import shap"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier, plot_tree\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import *\n","from xgboost import XGBClassifier, DMatrix\n","import shap"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_ml = sl.load_table('dumb_ml').query('label.isin((0, 1, 3))')\n","df_ml = sl.load_table('ml').query('label.isin((0, 1, 3))')\n","df_ml['label'] = df_ml['label'].apply(lambda x: 1 if x == 3 else x)\n","legit_count, bot_count = df_ml['label'].value_counts()\n","print(f'{legit_count = }\\n{bot_count = }')\n","print(df_ml.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_ml.label.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_ml.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_ml = df_ml[['follow_count', 'follower_count', 'comments_likes', 'post_count', 'comment_time_difference', 'label', 'username']]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Split data\n","X = df_ml.drop(['label', 'username'], axis=1)\n","y = df_ml['label']\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","# X_train, X_test, y_train, y_test =X.iloc[:X.shape[0] // 5 * 4, :], X.iloc[X.shape[0] // 5 * 4:, :], y[:y.shape[0] // 5 * 4], y[y.shape[0] // 5 * 4:]\n","\n","# Create a random forest classifier\n","clf = DecisionTreeClassifier(max_depth=1000, criterion='entropy', random_state=42)\n","clf = RandomForestClassifier(n_estimators=1000, random_state=42)\n","clf = XGBClassifier(n_estimators=1000,        \n","                    n_jobs=-1,\n","                    eval_metric='auc',\n","                    random_state=42\n","                    )\n","\n","clf.fit(X_train, \n","        y_train\n","        )\n","\n","# clf.get_params()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# for x in range(1, 10):\n","decision_threshold = sm.threshold_finder(model = clf, X = X_test, y_true = y_test)\n","print(decision_threshold)\n","y_pred_proba = [x[1] for x in clf.predict_proba(X_test)]\n","y_pred = (pd.Series(y_pred_proba) > decision_threshold).astype(int)\n","\n","y_pred_train = clf.predict(X_train)\n","y_pred_test = clf.predict(X_test)\n","\n","# Test the classifier on the test data\n","for metric in [accuracy_score, precision_score, recall_score, f1_score, average_precision_score, roc_auc_score]:\n","    \n","    try: \n","        score = metric(y_test, y_pred, average='weighted')\n","        score_inverse = metric(y_test.eq(0), y_pred.eq(0), average='weighted')\n","        diff = metric(y_train, y_pred_train) - metric(y_test, y_pred, average='weighted')\n","        diff_inverse = metric(y_train.eq(0), pd.Series(y_pred_train).eq(0)) - metric(y_test.eq(0), y_pred.eq(0), average='weighted')\n","        w = 'Weighted'\n","    except: \n","        score = metric(y_test, y_pred)\n","        score_inverse = metric(y_test.eq(0), y_pred.eq(0))\n","        diff = metric(y_train, y_pred_train) - metric(y_test, y_pred)\n","        diff_inverse = metric(y_train.eq(0), pd.Series(y_pred_train).eq(0)) - metric(y_test.eq(0), y_pred.eq(0))\n","        w = ''\n","\n","    print(w, f'{metric.__name__.title()}: {score:.4f}\\tDiff: {diff:.4f}\\t\\tScore inverse: {score_inverse:.4f}\\t{diff_inverse:.4f}')\n","\n","# print('\\n\\n\\n\\n\\n\\n\\n\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# sdaf"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Model explainer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import shap"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import xgboost as xgb\n","from xgboost import plot_importance, plot_tree, plotting\n","\n","import dtreeviz\n","import graphviz\n","import matplotlib.pyplot as plt\n","from matplotlib.pylab import rcParams\n","\n","import pandas as pd\n","import numpy as np\n","\n","%config InlineBackend.figure_format = 'retina' # Make visualizations look good\n","#%config InlineBackend.figure_format = 'svg' \n","%matplotlib inline\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# if type(clf) == RandomForestClassifier: \n","#     explainer = shap.TreeExplainer(clf)\n","#     shap_values = explainer(X)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# shap_values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# explainer.expected_value"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# shap.dependence_plot(1, shap_values, X)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# clf =="]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["explainer = shap.TreeExplainer(clf)\n","shap_values = explainer.shap_values(X)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# asdf"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# X,y = shap.datasets.adult()\n","# X_display,y_display = shap.datasets.adult(display=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# display(shap.summary_plot(shap_values, X_test))\n","# display(shap.summary_plot(shap_values, X_test, plot_type=\"bar\"))\n","# display(shap.summary_plot(shap_values, X_test, plot_type=\"dot\"))\n","# display(shap.dependence_plot(\"feature_name\", shap_values, X_test))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# explainer = ClassifierExplainer(clf, X_test, y_test, \n","#                                 cats=['Deck', 'Embarked',\n","#                                     {'Gender': ['Sex_male', 'Sex_female', 'Sex_nan']}],\n","#                                 cats_notencoded={'Embarked': 'Stowaway'}, # defaults to 'NOT_ENCODED'\n","#                                 # descriptions=feature_descriptions, # adds a table and hover labels to dashboard\n","#                                 labels=['Not survived', 'Survived'], # defaults to ['0', '1', etc]\n","#                                 idxs = X_test.columns, # defaults to X.index\n","#                                 index_name = \"Passenger\", # defaults to X.index.name\n","#                                 target = \"Survival\", # defaults to y.name\n","#                                 )\n","\n","# db = ExplainerDashboard(explainer, \n","#                         title=\"Titanic Explainer\", # defaults to \"Model Explainer\"\n","#                         shap_interaction=False, # you can switch off tabs with bools\n","#                         )\n","# db.run(port=8050)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# viz_model = dtreeviz.model(clf, tree_index=1,\n","#                            X_train=X_train, y_train=y_train,\n","#                            feature_names=X_train.columns,\n","#                            target_name=['label'], class_names=[0, 1])\n","\n","# viz_model.view()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# viz_model.view()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Shap"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# asdf"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["xgb_explainer = shap.TreeExplainer(\n","    clf, X, feature_names=X.columns.tolist()\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["booster_xgb = clf.get_booster()\n","shap_values_xgb = booster_xgb.predict(DMatrix(X, y), \n","                                      pred_contribs=True)\n","\n","shap_values_xgb = shap_values_xgb[:, :-1]\n","\n","pd.DataFrame(shap_values_xgb, columns=X_train.columns.tolist()).head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pd.DataFrame(shap_values_xgb, columns=X.columns.tolist()).head().median(axis=0).abs().sort_values(ascending=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pd.DataFrame(shap_values_xgb, columns=X.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# asdf"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["shap.summary_plot(shap_values_xgb, X, feature_names=X_train.columns, plot_type=\"bar\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["shap.summary_plot(shap_values_xgb, X, feature_names=X_train.columns);"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["shap.initjs()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["shap.force_plot(explainer.expected_value, shap_values[:1000,:], X.iloc[:1000,:])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["shap.approximate_interactions(col, shap_values, X)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# for column in df_ml: \n","#     if column in ('username'):\n","#         continue\n","#     display(shap.dependence_plot(column, shap_values_xgb, X, interaction_index=\"auto\"))\n","\n","\n","\n","for col in ['follow_count', 'comments_likes', 'follower_count']:\n","    # for x in (0, 1):\n","        # if x == 11: \n","        #     shap.dependence_plot(f'rank({i})', shap_values, X, interaction_index=None)\n","        # else: \n","    inds = shap.approximate_interactions(col, shap_values, X)\n","\n","    # make plots colored by each of the top three possible interacting features\n","    for i in range(3):\n","        shap.dependence_plot(col, shap_values, X, interaction_index=inds[i])\n","\n","    # figure = pl.gcf()\n","        pl.savefig(f\"{col}{i}.svg\",dpi=700)\n","    # display(f.show())\n","    # break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# interactions_xgb = booster_xgb.predict(DMatrix(X, y), pred_interactions=True\n","# )\n","\n","# shap_values_xgb = booster_xgb.predict(DMatrix(X, y), pred_contribs=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["shap_values_xgb = booster_xgb.predict(DMatrix(X, y), \n","                                      pred_contribs=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_top_k_interactions(feature_names, shap_interactions, k):\n","    # Get the mean absolute contribution for each feature interaction\n","    aggregate_interactions = np.mean(np.abs(shap_interactions[:, :-1, :-1]), axis=0)\n","    interactions = []\n","    for i in range(aggregate_interactions.shape[0]):\n","        for j in range(aggregate_interactions.shape[1]):\n","            if j < i:\n","                interactions.append(\n","                    (\n","                        feature_names[i] + \"-\" + feature_names[j],\n","                        aggregate_interactions[i][j] * 2,\n","                    )\n","                )\n","    # sort by magnitude\n","    interactions.sort(key=lambda x: x[1], reverse=True)\n","    interaction_features, interaction_values = map(tuple, zip(*interactions))\n","\n","    return interaction_features[:k], interaction_values[:k]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["top_10_inter_feats, top_10_inter_vals = get_top_k_interactions(\n","    X_train.columns, interactions_xgb, 10\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["top_10_inter_feats"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["top_10_inter_vals"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","def plot_interaction_pairs(pairs, values):\n","    plt.bar(pairs, values)\n","    plt.xticks(rotation=90)\n","    plt.tight_layout()\n","    plt.show();\n","    \n","\n","top_10_inter_feats, top_10_inter_vals = get_top_k_interactions(\n","    X.columns, interactions_xgb, 10\n",")\n","\n","plot_interaction_pairs(top_10_inter_feats, top_10_inter_vals)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["os.system('say \"ml done\"')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Playground"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## 0.x Mid playground"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### III finding outliers"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_main.select_dtypes(['int', 'float']).head(1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# for x in df_main.select_dtypes(['int', 'float']): \n","#     print(f\"'{x}',\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def outliers_bound(self, col):\n","    q1, q3 = self[col].quantile([.25, .75])\n","    iqr = q3 - q1\n","    lower_bound = q1 - (1.5 * iqr)\n","    upper_bound = q3 + (1.5 * iqr)\n","    return lower_bound, upper_bound\n","\n","def find_outliers(self, col, bounds=[]):\n","    '''Given a columns, return a pd.Series with True/False for each value'''\n","    if len(bounds) == 2: \n","        lower_bound, upper_bound = outliers_bound(col)\n","    else: \n","        lower_bound, upper_bound = bounds\n","\n","    return self[col].fillna(self[col].quantile(.5)).apply(lambda x: lower_bound <= x <= upper_bound if pd.notna(x) else False)\n","\n","\n","def find_all_outliers(self, dtypes=None, columns=None, exclude=[]):\n","    # Add or remove column to the list\n","    if dtypes != None: \n","        columns = list(self.select_dtypes(dtypes))\n","    elif isinstance(columns, str):\n","        columns = [columns]\n","    for col in exclude: \n","        columns.remove(col)\n","\n","    # Find outliers and concat\n","    base_series = pd.Series(['']*self.shape[0])\n","    for index, col in enumerate(columns): \n","        col_outliers = find_outliers(self, col).map({True: col, False: ''})\n","        base_series = base_series.str.cat(col_outliers, sep=' ').str.strip()\n","        \n","    return base_series\n","    \n","\n","# columns = ['follow_count', 'follower_count', 'comments_likes', 'comment_time_difference',  'post_count',  'video_count',  'highlight_reel_count',  'posts_minutes_diff']\n","# df_main['has_outliers'] = find_all_outliers(df_main, columns=columns)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# find_outliers(df_main, 'follow_count').value_counts()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### III dendrogram"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","from matplotlib import pyplot as plt\n","from scipy.cluster.hierarchy import dendrogram, linkage\n","\n","\n","# Calculate the distance between each sample\n","# You have to think about the metric you use (how to measure similarity) + about the method of clusterization you use (How to group cars)\n","Z = linkage(df_main[[col for col in df_main.columns if 'post' not in col]].select_dtypes(['int', 'float']), 'ward')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"main_env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"06d12861cad82bdcdde1b56bd9eda52e91f7df29dabbeda8f3d9112222750302"}}},"nbformat":4,"nbformat_minor":2}
