{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook description\n",
    "\n",
    "#### What is this notebook for? \n",
    "This this the first notebook of a serie of five. Its main purpose is to collect the data that will be used for the rest of the project. \n",
    "\n",
    "\n",
    "The process of data collection will be divided in multiple processes: \n",
    "1. Create a list of pages/account where I found bots' comments \n",
    "2. For each, I look at their last 50+ posts to collect the ID of the post \n",
    "3. Once done, loop through those thousands of IDs to collect the comments\n",
    "4. \n",
    "\n",
    "\n",
    "\n",
    "collected the IDs of  from multiple pages suseptible to have bots commenting on their posts. I selected those pages manually by making sure they're all bots' targets. Then, I'll loop through each post thanks to it's ID. On instagram, each post url is made as `instagram.com/p/{postid}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules and UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Personnal module, functions I use often\n",
    "import src.webscraping as mw\n",
    "import src.useful as mu\n",
    "\n",
    "\n",
    "# Better print \n",
    "from tqdm import tqdm\n",
    "\n",
    "# To become Dr Strange \n",
    "import time \n",
    "from datetime import datetime\n",
    "\n",
    "# Basic data \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Store data \n",
    "import sqlite3\n",
    "import json\n",
    "from flatten_json import flatten\n",
    "\n",
    "# Move things around locally\n",
    "import shutil\n",
    "\n",
    "# Fetch instagram data \n",
    "from instaloader import Instaloader, Profile\n",
    "from instaloader.exceptions import ProfileNotExistsException\n",
    "import urllib\n",
    "from splinter import Browser\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting notebook preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating SQL database to store all the data for this project\n",
    "database = \"data/main_database.sqlite\"\n",
    "con = sqlite3.connect(database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Collect post ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List a number of instagram accounts that have bots commenting on their posts. Here I'm looping through a page list that is targeted by bots and collect the posts_ids one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# During the first collection time I had 5 more\n",
    "pages = [\"nfl\", \"championsleague\", \"mercedesamgf1\", \"ESPN\", \"bleacherreport\", \"houseofhighlights\", \"nba\", \"worldstar\", \"grmdaily\", \"pubity\", \n",
    "         \"meme.ig\", \"brgridiron\", \"lakers\", \"ballislife\", \"nflonfox\", \"nflnetwork\", \"espnnfl\", \"cbssports\", \"thecheckdown\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post_ids = pd.DataFrame(columns=[\"post_id\", \"page\"])\n",
    "post_per_page = 50\n",
    "\n",
    "# Lunch browser\n",
    "browser = Browser('chrome')\n",
    "\n",
    "for page in pages:\n",
    "    browser.visit(f\"https://www.instagram.com/{page}/\")\n",
    "    postids = []\n",
    "    for i in list(range(10))[::-1]:\n",
    "        print(i, end=\"\\r\")\n",
    "        time.sleep(1)\n",
    "\n",
    "    while len(postids) < post_per_page:\n",
    "        # Scroll up and then down each time helps the page to not bug\n",
    "        browser.execute_script(\"window.scrollTo(0, 0);\")\n",
    "        time.sleep(0.5)\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Change the browser to a beautiful soup object where I can get the posts id\n",
    "        soup = mw.bsoup(browser)\n",
    "\n",
    "        # Instagram doesn't show in the html the posts it of the post we don't see so I need to slowly scroll down to collect each of them.\n",
    "        browser.execute_script(\"window.scrollBy(0, 200);\")\n",
    "\n",
    "        for element in soup.find_all(\"a\"):\n",
    "            link = element.get(\"href\")\n",
    "            \n",
    "            # We can find the posts id by looking into a tags that have an attribute of href.\n",
    "            if \"/p/\" in link:\n",
    "                postids.append(link.replace(\"/p/\", \"\").replace(\"/\", \"\"))\n",
    "\n",
    "        # Removing duplicates\n",
    "        postids = list(set(postids))\n",
    "\n",
    "        time.sleep(np.random.randint(30)/10)\n",
    "\n",
    "\n",
    "        post_page_id = [[post_id, page] for post_id in postids]\n",
    "\n",
    "        # Create a df with post_ids and save it in the db\n",
    "        df_new_post_ids = pd.DataFrame(post_page_id, columns=[\"post_id\", \"page_id\"])\n",
    "        df_new_post_ids.to_sql(\"post_ids\", con, if_exists=\"append\", index=False)\n",
    "        time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scrape comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>page_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CY23Dx-hQzz</td>\n",
       "      <td>br_hoops</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CYhycdhh__L</td>\n",
       "      <td>br_hoops</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CXpjIIagKAx</td>\n",
       "      <td>br_hoops</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CYxi2doBjWu</td>\n",
       "      <td>br_hoops</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CYDVbgjhJEq</td>\n",
       "      <td>br_hoops</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7826</th>\n",
       "      <td>CWIYygLtmsJ</td>\n",
       "      <td>mercedesamgf1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7827</th>\n",
       "      <td>CZhKJh_uJne</td>\n",
       "      <td>mercedesamgf1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7828</th>\n",
       "      <td>CWROwfUMZR4</td>\n",
       "      <td>mercedesamgf1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7829</th>\n",
       "      <td>CYWw8V4tRPD</td>\n",
       "      <td>mercedesamgf1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7830</th>\n",
       "      <td>CYi8IhRNilj</td>\n",
       "      <td>mercedesamgf1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7831 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          post_id      page_name\n",
       "0     CY23Dx-hQzz       br_hoops\n",
       "1     CYhycdhh__L       br_hoops\n",
       "2     CXpjIIagKAx       br_hoops\n",
       "3     CYxi2doBjWu       br_hoops\n",
       "4     CYDVbgjhJEq       br_hoops\n",
       "...           ...            ...\n",
       "7826  CWIYygLtmsJ  mercedesamgf1\n",
       "7827  CZhKJh_uJne  mercedesamgf1\n",
       "7828  CWROwfUMZR4  mercedesamgf1\n",
       "7829  CYWw8V4tRPD  mercedesamgf1\n",
       "7830  CYi8IhRNilj  mercedesamgf1\n",
       "\n",
       "[7831 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query post_id not in comments table \n",
    "query = \"\"\"\n",
    "SELECT  \n",
    "    DISTINCT\n",
    "    post_id\n",
    "    , page_name\n",
    "FROM post_ids \n",
    "\"\"\"\n",
    "\n",
    "# Loadind post_ids from db\n",
    "posts_ids_to_scrape = pd.read_sql_query(query, con)\n",
    "posts_ids_to_scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping and storing comments from post_ids:\n",
    "np.random.shuffle(posts_ids_to_scrape)\n",
    "for post_id in tqdm(posts_ids_to_scrape):\n",
    "    browser.visit(f\"https://www.instagram.com/p/{post_id}/\")\n",
    "    \n",
    "    while True: \n",
    "        # Get page soup \n",
    "        soup = mw.bsoup(browser)\n",
    "        if soup.find(\"h2\", class_=\"_a9zc\") != None: \n",
    "            break\n",
    "\n",
    "    # Get days since posted\n",
    "    post_posted_time = soup.find(\"time\", class_=\"_a9ze _a9zf\").get(\"datetime\")\n",
    "    now = datetime.now()\n",
    "    days_diff = (now - pd.to_datetime(post_posted_time[:-1])).days\n",
    "\n",
    "    # I only keep what was posted less than a month ago\n",
    "    if days_diff > 31: \n",
    "        continue\n",
    "    \n",
    "    df_post_comments = pd.DataFrame(columns=[\"post_id\", \"page\", \"legend\", \"post_posted_time\", \"username\", \n",
    "                                                       \"full_comment_data\", \"comment\", \"comment_posted_time\", \n",
    "                                                       \"time_since_posted\", \"comments_likes\", \"replies\",\n",
    "                                                       \"time_now\"])\n",
    "    for comment_block in soup.find_all(\"ul\", class_=\"_a9ym\"):\n",
    "        page = soup.find(\"h2\", class_=\"_a9zc\").text \n",
    "        legend = soup.find(\"div\", class_=\"_a9zs\").text \n",
    "        time_since_posted = comment_block.find(\"time\", class_=\"_a9ze _a9zf\").text \n",
    "        username = comment_block.find(\"h3\", class_=\"_a9zc\").text \n",
    "        comment_posted_time = comment_block.find(\"time\", class_=\"_a9ze _a9zf\").get(\"datetime\")\n",
    "        comments_likes = comment_block.find(\"button\", class_=\"_a9ze\").text \n",
    "        comment = comment_block.find(\"div\", class_=\"_a9zs\").text  \n",
    "        replies = comment_block.find(\"li\", class_=\"_a9yg\").text if comment_block.find(\"li\", class_=\"_a9yg\") != None else \"\"\n",
    "        full_comment_data = comment_block.text\n",
    "        time_now = datetime.now()\n",
    "\n",
    "        # Add comment values to dataframe\n",
    "        df_post_comments.loc[len(df_post_comments)] = [post_id, page, legend, post_posted_time, username, \n",
    "                                                       full_comment_data, comment, comment_posted_time, \n",
    "                                                       time_since_posted, comments_likes, replies,\n",
    "                                                       time_now]\n",
    "\n",
    "        df_post_comments[\"period\"] = \"summer\"\n",
    "\n",
    "        \n",
    "    df_post_comments.to_sql(\"comments\", con, if_exists=\"append\", index=False)\n",
    "    time.sleep(np.random.randint(40, 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Collect user data from comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciate Instaloader\n",
    "L = Instaloader()\n",
    "\n",
    "def fetch_user_data (username):\n",
    "    '''Function to fetch an Instagram user's public data.\n",
    "\n",
    "    Parameter: \n",
    "        username str: username of the user to collect the \n",
    "        data from.\n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        profile = Profile.from_username(L.context, username)\n",
    "    except ProfileNotExistsException:\n",
    "        return f\"{username} does not exists anymore\"\n",
    "        \n",
    "    data = profile.__dict__\n",
    "    del data[\"_context\"]\n",
    "    json_object = json.dumps(data, indent = 2)   \n",
    "    with open(f\"data/users_json/{username}_user_profile_data.json\", 'w') as file_object:  \n",
    "        json.dump(json_object, file_object) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_json (username):\n",
    "    # Open json \n",
    "    try:\n",
    "        json_file = open(f\"data/users_json/{username}_user_profile_data.json\")\n",
    "        data = json.loads(json.load(json_file))[\"_node\"]\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        return\n",
    "\n",
    "\n",
    "    # useles json keys\n",
    "    ban = ['edge_felix_video_timeline', 'edge_owner_to_timeline_media', 'edge_saved_media', \n",
    "            'edge_media_collections', 'edge_related_profiles']\n",
    "\n",
    "    # Defining basic keys. Those are name, follower count, bio, etc. Basic infos\n",
    "    basic_keys = [key for key in data.keys() if key not in ban]\n",
    "\n",
    "    basic_info = flatten({key:data[key] for key in basic_keys})\n",
    "    df_current_user = pd.DataFrame([basic_info])\n",
    "    # username = df_current_user.loc[0, \"username\"]\n",
    "\n",
    "    # Getting the data for all posts that are contained in lists. \n",
    "    df_current_user[\"video_count\"] = data['edge_felix_video_timeline'][\"count\"]\n",
    "    df_current_user[\"post_count\"] = data['edge_owner_to_timeline_media'][\"count\"]\n",
    "\n",
    "    last_12_posts = dict()\n",
    "    posts = data['edge_owner_to_timeline_media'][\"edges\"]\n",
    "    last_12_posts[\"username\"] = data[\"username\"]\n",
    "    last_12_posts[\"video_views\"] = [post[\"node\"][\"video_view_count\"] if \"video_view_count\" in post[\"node\"].keys() else np.nan for post in posts]\n",
    "    last_12_posts[\"display_url\"] = [post[\"node\"][\"display_url\"] for post in posts]\n",
    "    last_12_posts[\"thumbnail_src\"] = [post[\"node\"][\"thumbnail_src\"] for post in posts]\n",
    "    last_12_posts[\"accessibility_caption\"] = [post[\"node\"][\"accessibility_caption\"] for post in posts]\n",
    "    last_12_posts[\"is_video\"] = [post[\"node\"][\"is_video\"] for post in posts]\n",
    "    last_12_posts[\"likes\"] = [post[\"node\"][\"edge_liked_by\"][\"count\"] for post in posts]\n",
    "    last_12_posts[\"comments\"] = [post[\"node\"][\"edge_media_to_comment\"][\"count\"] for post in posts]\n",
    "    last_12_posts[\"timestamp\"] = [post[\"node\"][\"taken_at_timestamp\"] for post in posts]\n",
    "    df_last_12_posts = pd.DataFrame(last_12_posts)\n",
    "\n",
    "\n",
    "    # Changing list type to str as sqlite3 doesn't accept this type\n",
    "    list_features = ['bio_links', 'biography_with_entities_entities', 'edge_mutual_followed_by_edges', 'pronouns']\n",
    "    for column in list_features: \n",
    "        if column in df_current_user.columns:\n",
    "            df_current_user[column] = df_current_user[column].apply(lambda x:  \"_LIST_SEPARATOR_\".join(x))\n",
    "\n",
    "    return df_current_user, df_last_12_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query post_id not in comments table \n",
    "query = \"\"\"\n",
    "SELECT  \n",
    "    DISTINCT\n",
    "    username\n",
    "FROM comments \n",
    "WHERE 1=1\n",
    "    --AND username NOT IN (SELECT username FROM users)\n",
    "    --AND username NOT IN (SELECT username FROM errors where error_type='user_not_found')\"\"\"\n",
    "\n",
    "# Loading usernames to scrape from db\n",
    "usernames_to_scrape = pd.read_sql_query(query, con)[\"username\"]\n",
    "print(f\"Usernames to scrape: {len(usernames_to_scrape)}\")\n",
    "\n",
    "\n",
    "for index, username in enumerate(tqdm(usernames_to_scrape[:5])): \n",
    "    # print(username, username, username, username)\n",
    "    fetch_user_data(username)\n",
    "    \n",
    "    try:\n",
    "        df_current_user, df_current_last_12_posts = convert_json(username)\n",
    "    except FileNotFoundError: # If an error happened while fetching the data, no file\n",
    "        continue\n",
    "\n",
    "    # Not all users have the same number of columns returned and SQL needs same cols so got to use this way\n",
    "    if index != 0: # No table exists before first index\n",
    "        df_users = pd.read_sql_query('select * from users', con)\n",
    "    df_users = pd.concat([df_users, df_current_user]).drop_duplicates()\n",
    "    df_users.to_sql(\"users\", con, if_exists=\"replace\", index=False)\n",
    "    df_current_last_12_posts.to_sql(\"last_12_posts\", con, if_exists=\"append\", index=False)\n",
    "\n",
    "\n",
    "    # Profile pic\n",
    "    for username, profile_pic_url in df_current_user[[\"username\",  \"profile_pic_url\"]].values: \n",
    "        try:\n",
    "            urllib.request.urlretrieve(profile_pic_url, f\"data/photos/user_profile_pictures/{username}_pp_user_photo.png\")\n",
    "        except Exception as e:\n",
    "            print(username, e, end='\\r')\n",
    "\n",
    "    # Last 12 posts\n",
    "    df_current_last_12_posts = df_current_last_12_posts.reset_index()\n",
    "    for username, display_url, index in df_current_last_12_posts[[\"username\", \"display_url\", \"index\"]].values: \n",
    "        urllib.request.urlretrieve(display_url, f\"data/photos/user_last_12_posts/{username}_{index}_user_photo.png\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Screenshot bio url landing page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>external_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mr.145th</td>\n",
       "      <td>https://onlyfans.com/letmeholdthatsoul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bmw122787</td>\n",
       "      <td>https://bit.ly/3qrZahY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stepan_ryabenko</td>\n",
       "      <td>https://teleg.run/stepan_ryabenko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>supersonicadu</td>\n",
       "      <td>http://www.youtube.com/user/TheSonic2124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lauren_viglietta</td>\n",
       "      <td>https://vs.co/7f9fa25d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19160</th>\n",
       "      <td>joshlozano_</td>\n",
       "      <td>http://wer1hoops.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19161</th>\n",
       "      <td>qurmaca</td>\n",
       "      <td>https://t.me/qurmaca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19162</th>\n",
       "      <td>crossedcourt</td>\n",
       "      <td>http://crossedcourt.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19163</th>\n",
       "      <td>lessy.morgan63</td>\n",
       "      <td>https://elastic-joliot-e02363.netlify.app/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19164</th>\n",
       "      <td>ktone5</td>\n",
       "      <td>https://youtu.be/dETztrPw2jk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19165 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               username                                external_url\n",
       "0              mr.145th      https://onlyfans.com/letmeholdthatsoul\n",
       "1             bmw122787                      https://bit.ly/3qrZahY\n",
       "2       stepan_ryabenko           https://teleg.run/stepan_ryabenko\n",
       "3         supersonicadu    http://www.youtube.com/user/TheSonic2124\n",
       "4      lauren_viglietta                      https://vs.co/7f9fa25d\n",
       "...                 ...                                         ...\n",
       "19160       joshlozano_                       http://wer1hoops.com/\n",
       "19161           qurmaca                        https://t.me/qurmaca\n",
       "19162      crossedcourt                    http://crossedcourt.com/\n",
       "19163    lessy.morgan63  https://elastic-joliot-e02363.netlify.app/\n",
       "19164            ktone5                https://youtu.be/dETztrPw2jk\n",
       "\n",
       "[19165 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query users having a link and remove those with NA\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    DISTINCT \n",
    "    username\n",
    "    , external_url \n",
    "FROM users \n",
    "WHERE external_url IS NOT NULL\"\"\"\n",
    "\n",
    "df_user_urls = pd.read_sql_query(query, con)\n",
    "df_user_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36/36 [02:28<00:00,  4.12s/it]\n"
     ]
    }
   ],
   "source": [
    "browser = Browser(\"chrome\")\n",
    "for username, external_url in tqdm(df_user_urls.values):\n",
    "    try:\n",
    "        browser.visit(external_url)\n",
    "        browser.driver.save_screenshot(f\"data/url_screenshot/{username}_external_url_screenshot.png\")\n",
    "        browser = mw.launch_driver(\"/Users/marclamy/Desktop/main file/code/igbot_final/chromedriver\")\n",
    "    except:\n",
    "        ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('main_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "06d12861cad82bdcdde1b56bd9eda52e91f7df29dabbeda8f3d9112222750302"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
